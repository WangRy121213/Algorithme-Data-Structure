{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList = [['my','dog','has','flea','problems','help','please'],\n",
    "                   ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "                  ['my','dalmation','is','so','cute','I','love','him'],\n",
    "                  ['stop','posting','stupid','worthless','garbage'],\n",
    "                  ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "                  ['quit','buying','worthless','dog','food','stupid']]\n",
    "    # 1代表侮辱性文字，0代表正常言论\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])#创建一个空集合\n",
    "    for document in dataSet:#创建两个集合的并集\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word : %s is not in my Vocabulary!\"%word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "postingList,classVec = loadDataSet()\n",
    "MyVocabList = createVocabList(postingList)\n",
    "testEntry = ['love','my','dalmation']\n",
    "thisDoc = array(setOfWords2Vec(MyVocabList,testEntry))\n",
    "print(thisDoc,)\n",
    "trainMat = []\n",
    "for postinDoc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(MyVocabList,postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import*\n",
    "def trainNBO(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords);p1Num = ones(numWords)# 若其中一个概率值为0，那么最后的乘积也为0\n",
    "    p0Denom = 2.0;p1Denom = 2.0                  #  为降低这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)\n",
    "    p0Vect = log(p0Num/p0Denom)\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936\n",
      " -3.25809654 -2.56494936 -3.25809654 -2.15948425 -3.25809654 -1.87180218\n",
      " -3.25809654 -3.25809654 -3.25809654 -2.56494936 -2.56494936 -2.56494936\n",
      " -3.25809654 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936]\n",
      "[-3.04452244 -3.04452244 -3.04452244 -1.65822808 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244\n",
      " -1.94591015 -3.04452244 -2.35137526 -2.35137526 -2.35137526 -3.04452244\n",
      " -2.35137526 -2.35137526 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -2.35137526 -3.04452244 -1.94591015 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "p0Vect,p1Vect,pAbusive = trainNBO(trainMat,classVec)\n",
    "print(p0Vect,)\n",
    "print(p1Vect,)\n",
    "print(pAbusive,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify,p0Vect,p1Vect,pClass1):\n",
    "    p1 = sum(vec2Classify*p1Vect) + log(pClass1)\n",
    "    p0 = sum(vec2Classify*p0Vect) + log(1.0-pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    postingList,classVec = loadDataSet()\n",
    "    MyVocabList = createVocabList(postingList)\n",
    "    trainMat = []\n",
    "    for postinDoc in postingList:\n",
    "        trainMat.append(setOfWords2Vec(MyVocabList,postinDoc))\n",
    "    p0Vect,p1Vect,pAbusive = trainNBO(trainMat,classVec)\n",
    "    testEntry = ['love','my','dalmation']\n",
    "    thisDoc = setOfWords2Vec(MyVocabList,testEntry)\n",
    "    print (testEntry,'classified as:',classifyNB(thisDoc,p0Vect,p1Vect,pAbusive))\n",
    "    testEntry = ['stupid','garbage']\n",
    "    thisDoc = setOfWords2Vec(MyVocabList,testEntry)\n",
    "    print (testEntry,'classified as:',classifyNB(thisDoc,p0Vect,p1Vect,pAbusive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as: 0\n",
      "['stupid', 'garbage'] classified as: 1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList,inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用朴素贝叶斯来过滤垃圾邮件\n",
    "   第一步：用函数spli 与 正则表达式来切分句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'since', 'you', 'are', 'an', 'owner', 'of', 'at', 'least', 'one', 'google', 'groups', 'group', 'that', 'uses', 'the', 'customized', 'welcome', 'message', 'pages', 'or', 'files', 'we', 'are', 'writing', 'to', 'inform', 'you', 'that', 'we', 'will', 'no', 'longer', 'be', 'supporting', 'these', 'features', 'starting', 'february', '2011', 'we', 'made', 'this', 'decision', 'so', 'that', 'we', 'can', 'focus', 'on', 'improving', 'the', 'core', 'functionalities', 'of', 'google', 'groups', 'mailing', 'lists', 'and', 'forum', 'discussions', 'instead', 'of', 'these', 'features', 'we', 'encourage', 'you', 'to', 'use', 'products', 'that', 'are', 'designed', 'specifically', 'for', 'file', 'storage', 'and', 'page', 'creation', 'such', 'as', 'google', 'docs', 'and', 'google', 'sites', 'for', 'example', 'you', 'can', 'easily', 'create', 'your', 'pages', 'on', 'google', 'sites', 'and', 'share', 'the', 'site', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '174623', 'with', 'the', 'members', 'of', 'your', 'group', 'you', 'can', 'also', 'store', 'your', 'files', 'on', 'the', 'site', 'by', 'attaching', 'files', 'to', 'pages', 'http', 'www', 'google', 'com', 'support', 'sites', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '90563', 'on', 'the', 'site', 'if', 'you', 're', 'just', 'looking', 'for', 'a', 'place', 'to', 'upload', 'your', 'files', 'so', 'that', 'your', 'group', 'members', 'can', 'download', 'them', 'we', 'suggest', 'you', 'try', 'google', 'docs', 'you', 'can', 'upload', 'files', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '50092', 'and', 'share', 'access', 'with', 'either', 'a', 'group', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '66343', 'or', 'an', 'individual', 'http', 'docs', 'google', 'com', 'support', 'bin', 'answer', 'py', 'hl', 'en', 'answer', '86152', 'assigning', 'either', 'edit', 'or', 'download', 'only', 'access', 'to', 'the', 'files', 'you', 'have', 'received', 'this', 'mandatory', 'email', 'service', 'announcement', 'to', 'update', 'you', 'about', 'important', 'changes', 'to', 'google', 'groups']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def regE():\n",
    "    emailText = open('H:Desktop\\wangg.txt','r').read()\n",
    "    tok = []\n",
    "    regE = re.compile('[^0-9a-zA-Z]')\n",
    "    listOfTokens = regE.split(emailText)\n",
    "    for word in listOfTokens:\n",
    "        if len(word) > 0:\n",
    "            tok.append(word.lower())\n",
    "    return tok\n",
    "    emailText.close()\n",
    "print(regE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
